{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd_7R5YDv2PW"
      },
      "source": [
        "\n",
        "\n",
        "# Fine-Tuning the Llama 2 Model: An Experiment\n",
        "In this experiment, we primarily focus on fine-tuning the Llama 2 model. After the fine-tuning process, we will evaluate the model's performance by measuring its perplexity and cross-entropy. To enhance the results post fine-tuning, students are encouraged to adjust dataset sizes or tweak model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eqJlq-GvoHf"
      },
      "source": [
        "##  Load and Import Libraries\n",
        "\n",
        "Before diving into any deep learning or model fine-tuning tasks, it's essential to set up our environment correctly. This section ensures that all necessary libraries and dependencies are installed and ready to use.\n",
        "\n",
        "Here's a breakdown of the process:\n",
        "\n",
        "- **!pip install**: The `pip` command is used to install Python packages. The `!` at the beginning allows us to run shell commands directly from the notebook.\n",
        "\n",
        "  - **accelerate==0.21.0**: A library developed by Hugging Face to make distributed training and hardware acceleration easy.\n",
        "  \n",
        "  - **peft==0.4.0**: A library specific to LLaMa fine-tuning.\n",
        "  \n",
        "  - **bitsandbytes==0.40.2**: Assists in efficient training by utilizing 4-bit quantization for model weights.\n",
        "  \n",
        "  - **transformers==4.31.0**: The core library from Hugging Face that provides pre-trained models, tokenizers, and training utilities.\n",
        "  \n",
        "  - **trl==0.4.7**: A library for training reinforcement learning models.\n",
        "  \n",
        "  - **evaluate**: Presumably, a package that provides evaluation utilities (Note: ensure this package's relevance to your tasks).\n",
        "\n",
        "Ensure that all these libraries are successfully installed before proceeding. In case of any issues or conflicts, consider creating a virtual environment or seeking updated versions of the packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiFYLnWRuobl",
        "outputId": "8dfbe4c9-1f87-47f8-caae-e18d0f8ede08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/244.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0315ARsKwpub"
      },
      "source": [
        "After ensuring all required libraries are installed, the next step is to import the relevant classes and modules that will be used throughout our fine-tuning and testing process. Here's a brief overview of each import:\n",
        "\n",
        "- **os**: The built-in Python module that provides functionalities to interact with the operating system, mainly for file and directory operations.\n",
        "\n",
        "- **torch**: PyTorch is an open-source machine learning library widely used for deep learning tasks.\n",
        "\n",
        "- **datasets**: A library from Hugging Face for easily accessing and using datasets. Here, we specifically use the `load_dataset` function to load our dataset.\n",
        "\n",
        "- **transformers**: The core library from Hugging Face offering pre-trained models, tokenizers, and other utilities.\n",
        "  \n",
        "  - **AutoModelForCausalLM**: A class to instantiate models for causal (unidirectional) language modeling tasks.\n",
        "  \n",
        "  - **AutoTokenizer**: A class that provides tokenizers compatible with the pre-trained models.\n",
        "  \n",
        "  - **BitsAndBytesConfig**: Configuration class for 4-bit quantization from the bitsandbytes library.\n",
        "  \n",
        "  - **HfArgumentParser**: A parser specifically designed for Hugging Face libraries' arguments.\n",
        "  \n",
        "  - **TrainingArguments**: Defines training-related parameters such as batch size, learning rate, etc.\n",
        "  \n",
        "  - **pipeline**: Provides a high-level, easy-to-use API for performing tasks with models (e.g., text generation).\n",
        "  \n",
        "  - **logging**: A utility to control and handle logging behaviors.\n",
        "  \n",
        "- **peft**:\n",
        "  \n",
        "  - **LoraConfig**: Configuration for LLaMa's LoRA.\n",
        "  \n",
        "  - **PeftModel**: Not explicitly used in the provided code but could be a model variant for fine-tuning.\n",
        "  \n",
        "- **trl**: The library for training reinforcement learning models.\n",
        "  \n",
        "  - **SFTTrainer**: A trainer class designed for supervised fine-tuning tasks.\n",
        "\n",
        "It's essential to understand the purpose of each import as it provides context to the functionalities and utilities we will leverage in the subsequent steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkBETJLfvNWI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXbNIR9fwddC"
      },
      "source": [
        "## Model and Dataset Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcSpps3Swpuc"
      },
      "source": [
        "### Model Choice\n",
        "We have opted for the model named **NousResearch/llama-2-7b-chat-hf** as it's based on the Llama 2 architecture and has already been pre-trained across a variety of tasks, demonstrating commendable performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdx8JG-kwpuc"
      },
      "source": [
        "### Dataset Choice\n",
        "The dataset **mlabonne/guanaco-llama2-1k** was chosen specifically because it's tailored for the Llama 2 model and is suitable for the experiment at hand. Detailed information about this dataset can be found on the [Hugging Face Datasets Hub](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)\n",
        ". Depending on your GPU resources, you might opt for datasets of different sizes for fine-tuning. Larger datasets might offer better fine-tuning results but will also increase computational demands.\n",
        "\n",
        "**Note**: The size of the dataset and model parameters should be balanced based on the GPU resources available to you. Choosing a very large dataset or model might lead to GPU memory exhaustion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX_eka0Pwlwu"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "#model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
        "model_name =\"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-miniguanaco-6100\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFjHlGZwxOW"
      },
      "source": [
        "## QLoRa and bitsandbytes parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7fv5y3rf03a"
      },
      "source": [
        "QLoRA (Quantized LoRA) is a new approach that allows to finetune a very large language model using just a single big enough GPU - so it is suited for those who run notebooks on Colab! QLoRA reduces the average memory requirements of finetuning a 65B parameter model from >780GB of GPU memory to <48GB without degrading the runtime or predictive performance compared to a 16-\n",
        "bit fully finetuned baseline. This approach is based on the following two methods.\n",
        "\n",
        "\n",
        "(1) **Block-wise k-bit Quantization**: the input tensor is chunked into blocks that are independently quantized (quantization is the process of discretizing an input from a representation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers).\n",
        "\n",
        "(2) **LoRA (Low-Rank Adapters)**: a method that reduces memory requirements by using a small set of trainable parameters, often termed adapters, while not updating the full model parameters which remain fixed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKvgjwyMiXLW"
      },
      "source": [
        "QLoRa 参数：\n",
        "1.   lora_r：LoRA 注意力维度。它定义了LoRA层中的注意力头数。\n",
        "2.   lora_alpha：LoRA 缩放的 alpha 参数。它用于缩放LoRA层的注意力权重。\n",
        "3.   lora_dropout：LoRA 层的 dropout 概率。它定义了在LoRA层中进行训练时的节点丢弃概率。\n",
        "\n",
        "bitsandbytes 参数：\n",
        "\n",
        "\n",
        "1.  use_4bit：激活4位精度基础模型加载。当设置为True时，将使用4位精度的基础模型进行加载。\n",
        "2.  bnb_4bit_compute_dtype：4位基础模型的计算数据类型。在此设置中，使用float16作为计算数据类型。\n",
        "3.  bnb_4bit_quant_type：量化类型（fp4或nf4）。它指定了4位基础模型中使用的量化类型。\n",
        "4.  use_nested_quant：激活4位基础模型的嵌套量化（双重量化）。当设置为True时，将应用嵌套量化。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkRbeUcrw0tj"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEziwGLFw7tt"
      },
      "source": [
        "## Training Parameters Configuration\n",
        "\n",
        "This section outlines various parameters that influence the training process. By adjusting these parameters, you can experiment with and fine-tune the performance of your model. Familiarize yourself with each parameter's purpose, and consider modifying them as part of your learning experience.\n",
        "\n",
        "### TrainingArguments Parameters\n",
        "\n",
        "- **output_dir**: Directory where model predictions and checkpoints will be stored.\n",
        "- **num_train_epochs**: Number of training epochs.\n",
        "- **fp16** & **bf16**: Enable reduced precision training for faster computation.\n",
        "- **per_device_train_batch_size**: Batch size per GPU for training.\n",
        "- ... [and so on for each parameter]\n",
        "\n",
        "### SFT Parameters\n",
        "\n",
        "- **max_seq_length**: Maximum sequence length to use.\n",
        "- **packing**: Pack multiple short examples in the same input sequence for increased efficiency.\n",
        "- ... [and so on for each parameter]\n",
        "\n",
        "Remember, parameter tuning is an iterative process. Experiment with different values to see how they affect the performance of your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUOa6wzxyEDD"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine)\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 25\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmhOfVrJhSIv"
      },
      "source": [
        "## Dataset Loading, Splitting, and Evaluation Metrics Calculation\n",
        "\n",
        "This section of the code serves several purposes: Firstly, it loads a specified dataset. Upon loading, it then divides the dataset into three distinct sets: training, testing, and validation. Once the data is processed, the code also includes a function designed to compute evaluation metrics. Specifically, it calculates the perplexity and cross-entropy based on the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRYLr12cfwZF",
        "outputId": "270bd5e0-86f0-4d50-d6de-11f6f24e7fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 990\n",
            "Number of testing samples: 10\n"
          ]
        }
      ],
      "source": [
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "\n",
        "# - Loading a dataset\n",
        "# - Splitting it into training, testing, and validation sets\n",
        "# - Computing evaluation metrics (perplexity and cross-entropy) for model predictions\n",
        "\n",
        "# Function to compute evaluation metrics\n",
        "def compute_evaluation_metrics(prediction_data):\n",
        "    \"\"\"\n",
        "    Computes evaluation metrics for a given prediction.\n",
        "\n",
        "    Parameters:\n",
        "    - prediction_data (EvalPrediction): Contains the predictions and true labels.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing perplexity and cross_entropy.\n",
        "    \"\"\"\n",
        "    model_outputs = torch.from_numpy(prediction_data.predictions)\n",
        "    true_labels = torch.from_numpy(prediction_data.label_ids)\n",
        "    cross_entropy_loss = torch.nn.functional.cross_entropy(model_outputs.view(-1, tokenizer.vocab_size), true_labels.view(-1))\n",
        "\n",
        "    return {\n",
        "        'perplexity': math.exp(cross_entropy_loss),\n",
        "        'cross_entropy': cross_entropy_loss\n",
        "    }\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(dataset_name, split=\"train\")#修改占比\n",
        "\n",
        "# Splitting the Dataset\n",
        "# Given the potential constraints of using the T4 GPU on Colab, it's advisable to reduce the number of test cases\n",
        "# during the testing phase. This helps in preventing issues related to GPU resource limitations.\n",
        "train_test_split = dataset.train_test_split(test_size=0.01, shuffle=True, seed=2023)\n",
        "#如果更小train_test_split = dataset.train_test_split(test_size=0.001, shuffle=True, seed=2023)\n",
        "train_data = train_test_split[\"train\"]\n",
        "test_data = train_test_split[\"test\"]\n",
        "\n",
        "# Display the number of samples in each split\n",
        "print(f\"Number of training samples: {len(train_data)}\")\n",
        "print(f\"Number of testing samples: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8ay3SfX3iTc"
      },
      "source": [
        "## Configuring 4-bit Quantization with BitsAndBytes\n",
        "\n",
        "Quantization is a technique that reduces the numerical precision of model weights, thus making the model smaller and often faster at the expense of a slight reduction in accuracy. In this section, we're leveraging `BitsAndBytes` to apply 4-bit quantization.\n",
        "\n",
        "- **compute_dtype**: Determines the data type for computation, either `float16` or `float32`.\n",
        "- **bnb_config**: Configuration for `BitsAndBytes`, which includes parameters to specify the type of 4-bit quantization and the data type for computation.\n",
        "- **GPU compatibility check**: The code snippet also includes a check to determine if the GPU supports `bfloat16`, which can further accelerate training. If your GPU is compatible, consider enabling `bf16` training.\n",
        "\n",
        "Remember, using quantization and reduced-precision training can accelerate the training process, but it's essential to monitor the model's performance to ensure the accuracy remains acceptable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6vAjS4X3k1c"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yE2M7Eo4c2p"
      },
      "outputs": [],
      "source": [
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjQLRCd13_NS"
      },
      "source": [
        "##  Loading the Llama 2 Model with 4-bit Precision and its Tokenizer\n",
        "\n",
        "In this section, we're initializing the Llama 2 model with specific configurations and then loading its associated tokenizer.\n",
        "\n",
        "- **Model Loading**:\n",
        "  - **model_name**: The name of the pre-trained Llama 2 model from Hugging Face.\n",
        "  - **quantization_config**: This parameter applies the 4-bit quantization configuration to the model, enabling it to process data in reduced precision for faster computation.\n",
        "  - **device_map**: Specifies which GPU the model should be loaded on.\n",
        "  - Additional configurations, like `use_cache` and `pretraining_tp`, are set for efficient memory usage and to specify the number of prediction tasks, respectively.\n",
        "\n",
        "- **Tokenizer Loading**:\n",
        "  - After loading the base model, we proceed to load the tokenizer, which is essential for converting text into a format that can be understood by the model.\n",
        "  - **trust_remote_code**: This parameter ensures that any custom code associated with the tokenizer is executed.\n",
        "  - Padding configurations (`pad_token`, `padding_side`) are set to ensure sequences are properly aligned for model input, and to address any potential issues with reduced-precision training.\n",
        "\n",
        "By understanding and tweaking these configurations, you can further adapt and optimize the loading process to meet specific requirements or experiment with variations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e9c36d7f1e2e4da9b9c1e9caca27d019",
            "e731befb4a374af3aca2c680f30a894b",
            "2dd31043b0e5424c8391a928a466af10",
            "108ba3ea5567485080462d95a45742d1",
            "83a67b4d874644db94cf9b7e39a2bae6",
            "1d497fd7033843e38b8e5ca602422798",
            "e104d666d7e244a9aa7b4bbc8ba5d787",
            "f52fb7210dfa421188fa8a68fe95cb86",
            "2bacdaecce8c4806b051f2af59f6a3ab",
            "d36072d6fb5342de8ed51a4242276f31",
            "5a640ed1b64b4713aad30df4014f095a"
          ]
        },
        "id": "MiZ0Ta3T4toc",
        "outputId": "ef0c1b15-29df-4aeb-85e7-1c68470e8326"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9c36d7f1e2e4da9b9c1e9caca27d019"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiA3iOUzkscM"
      },
      "source": [
        "##  Loading the LoRA Configuration\n",
        "\n",
        "In this section, we're initializing the Llama 2 model with the **LoRA (Localized Re-parametrization Approximation)** configuration. This configuration is designed to improve the fine-tuning capabilities of the model, especially on smaller datasets, by introducing additional adaptable parameters. Here's a breakdown of the configuration parameters:\n",
        "\n",
        "- **lora_alpha**: Controls the scaling of the initial weights in the LoRA layers. A higher value may improve the ability to adapt to new tasks but can also potentially lead to overfitting.\n",
        "\n",
        "- **lora_dropout**: Specifies the dropout rate for the LoRA layers, which helps in preventing overfitting.\n",
        "\n",
        "- **r**: Represents the rank for the low-rank approximation in the LoRA layers. By defining the rank, you can control the complexity and adaptability of the LoRA layers.\n",
        "\n",
        "- **bias**: Determines the type of bias used in the LoRA layers. In our configuration, we've set it to \"none\", meaning no bias is used.\n",
        "\n",
        "- **task_type**: This is set to \"CAUSAL_LM\" indicating that our task is a causal language modeling task, which predicts the next word in a sequence based on the previous words.\n",
        "\n",
        "By understanding and adjusting these parameters, you can modify the LoRA configuration to fine-tune its behavior, balancing between model adaptability and the risk of overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FxwqfSYkvIN"
      },
      "outputs": [],
      "source": [
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn3AhxlOk1JU"
      },
      "source": [
        "## Setting the Training and Supervised Fine-Tuning (SFT) Parameters\n",
        "\n",
        "In this segment, we establish the foundational parameters that drive the training process of the Llama 2 model. Here's a brief overview:\n",
        "\n",
        "### **Training Parameters**\n",
        "\n",
        "- **output_dir**: The directory where model predictions and checkpoints will be stored.\n",
        "  \n",
        "- **num_train_epochs**: Defines the number of times the model will iterate over the entire dataset.\n",
        "\n",
        "- **per_device_train_batch_size**: Specifies the number of samples to work with in one update of model parameters.\n",
        "\n",
        "- **gradient_accumulation_steps**: Determines how many steps to take before updating the model's weights.\n",
        "\n",
        "- **optim**: The optimization algorithm used for updating the model's weights.\n",
        "\n",
        "- **save_steps** & **logging_steps**: Controls how frequently the model checkpoints are saved and logs are generated, respectively.\n",
        "\n",
        "- **learning_rate** & **weight_decay**: Set the rate at which the model learns and the decay applied to weights over time, respectively.\n",
        "\n",
        "- **fp16** & **bf16**: Options to enable 16-bit floating point or bfloat16 precision training, which can speed up the training process.\n",
        "\n",
        "- **max_grad_norm**: Implements gradient clipping to prevent exceedingly large gradient values that can destabilize the training.\n",
        "\n",
        "- **max_steps**: Overrides the `num_train_epochs` by specifying the exact number of training steps.\n",
        "\n",
        "- **warmup_ratio**: Indicates the fraction of steps used for a linear warmup from 0 to the set learning rate.\n",
        "\n",
        "- **group_by_length**: Groups sequences of similar lengths together, enhancing efficiency.\n",
        "\n",
        "- **lr_scheduler_type**: Determines the type of learning rate schedule, influencing how the learning rate changes during training.\n",
        "\n",
        "### **Supervised Fine-Tuning Parameters**\n",
        "\n",
        "The `SFTTrainer` is a specialized trainer optimized for Supervised Fine-Tuning:\n",
        "\n",
        "- **model**: The actual model to be trained.\n",
        "\n",
        "- **train_dataset** & **eval_dataset**: The datasets used for training and evaluation respectively.\n",
        "\n",
        "- **peft_config**: Refers to the previously set LoRA configuration.\n",
        "\n",
        "- **dataset_text_field**: Field name that contains the actual text data in the dataset.\n",
        "\n",
        "- **max_seq_length**: Specifies the maximum length of the sequences for processing.\n",
        "\n",
        "- **tokenizer**: Helps in converting text data into a format suitable for model processing.\n",
        "\n",
        "- **args**: The aforementioned training arguments.\n",
        "\n",
        "- **packing**: Determines if multiple short examples will be packed into a single input sequence.\n",
        "\n",
        "- **compute_metrics**: Specifies the function used to compute evaluation metrics for the model's predictions.\n",
        "\n",
        "With a clear understanding of these parameters, you can tailor the training and fine-tuning process to suit specific needs and constraints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "bd44a96bcd314e38a1b914d5d08d1345",
            "5441a7696a9048afa9d1104d214b18a1",
            "d41bb36573014e329991f0ec8564f6ae",
            "fcb88dd3391b4c9ba41b8fe659ba7b15",
            "5812012f4c0c4a43b0193a4a5c356e10",
            "f7941a1687234def8a3ebbc108e23697",
            "c5d7606703c645e5babd5c705f3dc116",
            "12ece3f91c204b2c88e89d282fb938a5",
            "1fde3315841f4b65a201ef5f438067d4",
            "92460b5d1186430598630b3063b084a3",
            "2e64b81aa01046328d4689b90daa6f5d",
            "ce6dc3fc2e2a4fd29bc5fe0909634521",
            "5e5d4aae3c3a4002af6fab3c82e57eb2",
            "be976886eaea48e1994d2f16a534bfdc",
            "c00e0cc8917a4c22a3c9522a2818f335",
            "33bdc60ab44d44179e530f543c3fddef",
            "441806314381481ab349f1dce38ef3a9",
            "6311f599fe7c43bdb372b0c28eb32e15",
            "cc2b1d0df83345f881cea1a34dbb0230",
            "6748de8e783d424bb4e37daf384b2737",
            "7f44ef6bfa904b468b774933384d48a2",
            "b20ce80c417246a39803efd1fda77203"
          ]
        },
        "id": "eh3KopOrk-N6",
        "outputId": "ec183c54-6faf-4414-9999-f1cdc02e84cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd44a96bcd314e38a1b914d5d08d1345"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce6dc3fc2e2a4fd29bc5fe0909634521"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        "    compute_metrics=compute_evaluation_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imtQ6lIelMBO"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "reRuP4P6lTID",
        "outputId": "ecb93105-b65d-43ce-8e91-c4d63788b106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [248/248 24:24, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.264000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.529400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.123100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.387100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.158900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.368300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>1.141400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.401500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>1.125800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh5_zDX8ldmf"
      },
      "source": [
        "The training can be very long, depending on the size of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "6I7R3UvZlfI4",
        "outputId": "05b76027-c87c-4591-bf4c-dd864e4b0c50"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzoWxheZspPl"
      },
      "source": [
        "##  Testing the Model\n",
        "\n",
        "After training our model, it's crucial to evaluate its capabilities. This segment is dedicated to testing the model using a sample prompt and observing its generated response.\n",
        "\n",
        "Here's a brief walkthrough:\n",
        "\n",
        "- **logging.set_verbosity(logging.CRITICAL)**: This line ensures that only critical logs are shown, ignoring less severe warnings. It's useful for a cleaner output.\n",
        "\n",
        "- **prompt**: The question or statement we want the model to respond to. For this instance, we're curious about \"What is LLAMA?\".\n",
        "\n",
        "- **pipeline**: Hugging Face's `pipeline` functionality provides a straightforward way to run specific tasks. Here, we're setting it up for \"text-generation\".\n",
        "\n",
        "  - **task**: Specifies the type of task. In this case, it's \"text-generation\".\n",
        "  - **model**: The trained model that we want to test.\n",
        "  - **tokenizer**: The tokenizer used during training, responsible for converting text into a format the model understands.\n",
        "  - **max_length**: The maximum length for the generated response. We've set it to 200 characters to ensure responses are concise yet meaningful.\n",
        "\n",
        "- **result**: Here, we invoke the pipeline with our prompt to get the model's response. We're wrapping the prompt with special tokens (`<s>[INST]` and `[/INST]`), guiding the model to understand that we're seeking instructional text.\n",
        "\n",
        "Finally, we print out the generated text to observe how well the model responds to our query.\n",
        "\n",
        "By testing the model with different prompts, you can gauge its strengths, weaknesses, and areas of improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5cAp64Uss-d",
        "outputId": "2273c2a2-0c05-4024-a742-422cf572ab1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] What is LLAMA? [/INST] LLAMA is a programming language that is designed to be easy to learn and use. LLAMA is a high-level, object-oriented language that is similar to Python. LLAMA is designed to be easy to use for beginners, but also has features that make it suitable for more experienced programmers. LLAMA is a free and open-source programming language. LLAMA is a programming language that is designed to be easy to learn and use. LLAMA is a high-level, object-oriented language that is similar to Python. LLAMA is designed to be easy to use for beginners, but also has features that make it suitable for more experienced programmers. LLAMA is a free and open-source programming language. LLAMA is a programming language that is designed to be easy to learn and use. LLAMA is a high-level, object\n"
          ]
        }
      ],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is LLAMA?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOSDp3cjwpuh"
      },
      "source": [
        "## Submit to Kaggle for Text Generation Evaluation\n",
        "The code provided below is set up to generate the necessary submission file for our Kaggle competition. After executing this code, you'll obtain a `submission.csv` file that contains the model's predictions. To evaluate how well your model performs, you must submit this generated file on the Kaggle competition page.\n",
        "\n",
        "🔗 [Submit your file here on Kaggle!](https://www.kaggle.com/competitions/dsaa-6100-finetune-llm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjxqouN9wpuh",
        "outputId": "ee981dfe-34c1-4413-9901-383e1319d398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1aJs4sFPtF8FilWVHw888hkb8AGiOTo27\n",
            "To: /content/test_file.csv\n",
            "100%|██████████| 6.33k/6.33k [00:00<00:00, 8.14MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'submission.csv' has been saved.\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# download file\n",
        "url = 'https://drive.google.com/uc?export=download&id=1aJs4sFPtF8FilWVHw888hkb8AGiOTo27'\n",
        "output = 'test_file.csv'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Load the test_file.csv\n",
        "test_df = pd.read_csv('test_file.csv')\n",
        "\n",
        "# List to store predictions\n",
        "predictions = []\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "\n",
        "# Loop through each row in the test DataFrame and make predictions\n",
        "for index, row in test_df.iterrows():\n",
        "    input_content = row['Input Content']\n",
        "    result = pipe(f\"{input_content}\")\n",
        "    predictions.append(result[0]['generated_text'])\n",
        "\n",
        "# Create a new DataFrame with Id and label\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_df['ID'],\n",
        "    'label': predictions\n",
        "})\n",
        "\n",
        "# Save the DataFrame to CSV file\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"File 'submission.csv' has been saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghA0HXorjyy3"
      },
      "source": [
        "## Evaluation Metrics Explained\n",
        "- eval_loss: Represents the difference between the model's predictions and the actual data. A lower loss typically indicates that the model's predictions are closer to the true labels.\n",
        "\n",
        "- eval_perplexity: Measures the uncertainty of the model's predictions. A lower perplexity means the model is more confident in its predictions.\n",
        "\n",
        "- eval_cross_entropy: The cross-entropy measures the difference between the model's predicted probabilities and the true labels. We want this value to be as low as possible.\n",
        "\n",
        "- eval_runtime: The time required to evaluate the model.\n",
        "\n",
        "- eval_samples_per_second: Shows how many samples the model can process per second, reflecting its speed.\n",
        "\n",
        "- eval_steps_per_second: The number of optimization steps executed per second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmlL6icajvmn",
        "outputId": "de9ede1e-2a6f-468d-c6de-f95de2de2071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.1400146484375,\n",
              " 'eval_perplexity': 3048038.686851266,\n",
              " 'eval_cross_entropy': 14.930008888244629,\n",
              " 'eval_runtime': 11.0083,\n",
              " 'eval_samples_per_second': 0.908,\n",
              " 'eval_steps_per_second': 0.182,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwa11NyOwpui"
      },
      "source": [
        "# Fine-Tuning Recommendations for Deep Learning Models\n",
        "\n",
        "Kaggle's evaluation process utilizes BLEU and Jaccard similarity as its primary metrics. While the default configuration outlined above offers a foundational baseline score, there is ample room to optimize and refine your model's performance. Especially in the nuanced domain of natural language processing, there exists a wide array of strategies and methods that can be harnessed to elevate your model's efficacy. Below, we've curated a set of proven recommendations accompanied by relevant code snippets to guide your fine-tuning process.\n",
        "\n",
        "For those who wish to broaden their horizons and delve deeper into more sophisticated models, datasets, and fine-tuning strategies, the Hugging Face platform is a valuable resource. Explore more by navigating to [huggingface.co](https://huggingface.co).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK8I7XK9wpui"
      },
      "source": [
        "## Switching Pre-trained Models\n",
        "\n",
        "At present, we're utilizing the **NousResearch/llama-2-7b-chat-hf** model. Venturing into other pre-trained models, particularly those tailored more closely to our specific task, may pave the way for enhanced performance. For a broader selection of models, we can visit [huggingface.co](https://huggingface.co).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IednVFpwpui"
      },
      "outputs": [],
      "source": [
        "model_name = \"ModelName\"\n",
        "\n",
        "#model_name =\"TinyPixel/Llama-2-7B-bf16-sharded\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have made some changes here."
      ],
      "metadata": {
        "id": "QEW1M0OIZzul"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyMEZ1Vowpui"
      },
      "source": [
        "📝 Note: The choice of a pre-trained model can influence the fine-tuning process significantly. Larger models have more parameters and might capture nuances better but may also require more resources and time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y49Yf5Rwpum"
      },
      "source": [
        "## Experimenting with Different Datasets for Fine-tuning\n",
        "Besides the dataset **mlabonne/guanaco-llama2-1k** that we are currently using, consider leveraging datasets from related domains or even combining multiple datasets to achieve a richer fine-tuning source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfxJ93VXwpum"
      },
      "outputs": [],
      "source": [
        "# Loading an additional dataset\n",
        "another_dataset = load_dataset(\"another_dataset_name\", split=\"train\")\n",
        "# Combining two datasets\n",
        "combined_dataset = concatenate_datasets([dataset, another_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHF6rXxrwpum"
      },
      "source": [
        "📝 Note: Using diverse and domain-specific datasets can lead to better generalization and task-specific performance improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-ck-fFPwpum"
      },
      "source": [
        "## Tweaking LoRA Parameters\n",
        "Experimenting with lora_r, lora_alpha, and lora_dropout values might result in enhanced fine-tuning outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS68IztWwpum"
      },
      "outputs": [],
      "source": [
        "lora_r = 128\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdPLYGTYwpum"
      },
      "source": [
        "📝 Note: LoRA (Low-Rank Adaptation) parameters control the trade-off between model flexibility and the amount of new information added during fine-tuning. Adjusting them requires monitoring performance closely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbFSwHqSwpum"
      },
      "source": [
        "## Modifying Optimizer and Learning Rate Schedule\n",
        "We're currently employing the paged_adamw_32bit optimizer with a constant learning rate schedule. Trying out different optimizers or learning rate adjustment strategies might be beneficial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-Q4qzb-wpum"
      },
      "outputs": [],
      "source": [
        "optim = \"adamw\"\n",
        "lr_scheduler_type = \"linear\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E80yEJVkwpum"
      },
      "source": [
        "📝 Note: The optimizer and its settings play a crucial role in model convergence and overall performance. Depending on the dataset's nature and size, different optimizers and learning rate schedules might be more effective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF1rTdBWwpun"
      },
      "source": [
        "## Refining Training Strategy\n",
        "Adjusting parameters such as gradient_accumulation_steps, max_grad_norm, weight_decay, and experimenting with different batch sizes and learning rates can potentially lead to more optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqVpf_ZXwpun"
      },
      "outputs": [],
      "source": [
        "gradient_accumulation_steps = 2\n",
        "max_grad_norm = 1.0\n",
        "weight_decay = 0.01\n",
        "per_device_train_batch_size = 2\n",
        "learning_rate = 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgayff6gwpun"
      },
      "source": [
        "📝 Note: The training strategy directly affects how the model updates its weights. Depending on the data's characteristics and the chosen pre-trained model, varying these parameters can lead to faster convergence or better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EJp633Jwpun"
      },
      "source": [
        "## Regularization\n",
        "\n",
        "Introducing dropout or weight decay can assist in preventing overfitting.\n",
        "\n",
        "Code example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA2J8ptLwpun"
      },
      "outputs": [],
      "source": [
        "model = YourModel(dropout_rate=0.3)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYlLytYPwpun"
      },
      "source": [
        "## Gradient Clipping\n",
        "\n",
        "To prevent gradient explosions, you can clip gradients.\n",
        "\n",
        "Code example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF7gYxM4wpun"
      },
      "outputs": [],
      "source": [
        "torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyacepegwpun"
      },
      "source": [
        "## Adjusting Maximum Generation Length\n",
        "\n",
        "Limiting the text length produced by the model can help it focus on shorter, more relevant answers.\n",
        "\n",
        "Code example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJa7V8AXwpuo"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMMuoxkfwpuo"
      },
      "source": [
        "## Temperature Tuning\n",
        "\n",
        "Temperature can affect the diversity of the model's outputs. A lower value (e.g., 0.2) will make the output more deterministic, while a higher value (e.g., 1.0) introduces more randomness.\n",
        "\n",
        "Code example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgT53ijRwpuo"
      },
      "outputs": [],
      "source": [
        "result = pipe(f\"{input_content}\", temperature=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmKjxhWywpuo"
      },
      "source": [
        "## Using Prefixes to Guide the Model\n",
        "\n",
        "Adding a prefix to the model's input can help steer its generation.\n",
        "\n",
        "Code example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs2SSBHXwpuo"
      },
      "outputs": [],
      "source": [
        "result = pipe(f\"Summarize: {input_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AtHNCH4wpuo"
      },
      "source": [
        "Experiment with the methods above and make multiple submissions to Kaggle to observe any improvements in your model's score. Continuous experimentation and fine-tuning are key to enhancing model performance!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8Q6fC99wpuo"
      },
      "source": [
        "# Versatility of Large Language Model\n",
        "Beyond text generation, large models are multifaceted tools trained to perform a plethora of tasks. From sentiment analysis, text classification, and named entity recognition to more advanced tasks like answering complex questions, generating code, and even describing images, the applications are vast and varied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoGqIixfwpuo"
      },
      "source": [
        "## Open Questions and Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77wf7tI_wpuo"
      },
      "source": [
        "The vast capabilities of these models present numerous open questions. How can they be best fine-tuned for niche applications? In what innovative ways can they be integrated into different industries or disciplines?\n",
        "\n",
        "We encourage you all to embark on this open exploration. There's so much potential yet to be harnessed, and sometimes the most groundbreaking discoveries come from the most unexpected experiments.\n",
        "\n",
        "As we wrap up this notebook, think of this as an invitation to delve deeper, to probe and to ponder. And when you uncover something new, or even just intriguing, do bring it to the fore for the community to see, learn, and build upon.\n",
        "\n",
        "The journey of exploration is always better when shared. Let's journey together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXRgqfrAwpup"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "384px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9c36d7f1e2e4da9b9c1e9caca27d019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e731befb4a374af3aca2c680f30a894b",
              "IPY_MODEL_2dd31043b0e5424c8391a928a466af10",
              "IPY_MODEL_108ba3ea5567485080462d95a45742d1"
            ],
            "layout": "IPY_MODEL_83a67b4d874644db94cf9b7e39a2bae6"
          }
        },
        "e731befb4a374af3aca2c680f30a894b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d497fd7033843e38b8e5ca602422798",
            "placeholder": "​",
            "style": "IPY_MODEL_e104d666d7e244a9aa7b4bbc8ba5d787",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2dd31043b0e5424c8391a928a466af10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f52fb7210dfa421188fa8a68fe95cb86",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bacdaecce8c4806b051f2af59f6a3ab",
            "value": 14
          }
        },
        "108ba3ea5567485080462d95a45742d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d36072d6fb5342de8ed51a4242276f31",
            "placeholder": "​",
            "style": "IPY_MODEL_5a640ed1b64b4713aad30df4014f095a",
            "value": " 14/14 [01:18&lt;00:00,  5.55s/it]"
          }
        },
        "83a67b4d874644db94cf9b7e39a2bae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d497fd7033843e38b8e5ca602422798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e104d666d7e244a9aa7b4bbc8ba5d787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f52fb7210dfa421188fa8a68fe95cb86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bacdaecce8c4806b051f2af59f6a3ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d36072d6fb5342de8ed51a4242276f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a640ed1b64b4713aad30df4014f095a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd44a96bcd314e38a1b914d5d08d1345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5441a7696a9048afa9d1104d214b18a1",
              "IPY_MODEL_d41bb36573014e329991f0ec8564f6ae",
              "IPY_MODEL_fcb88dd3391b4c9ba41b8fe659ba7b15"
            ],
            "layout": "IPY_MODEL_5812012f4c0c4a43b0193a4a5c356e10"
          }
        },
        "5441a7696a9048afa9d1104d214b18a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7941a1687234def8a3ebbc108e23697",
            "placeholder": "​",
            "style": "IPY_MODEL_c5d7606703c645e5babd5c705f3dc116",
            "value": "Map: 100%"
          }
        },
        "d41bb36573014e329991f0ec8564f6ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12ece3f91c204b2c88e89d282fb938a5",
            "max": 990,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fde3315841f4b65a201ef5f438067d4",
            "value": 990
          }
        },
        "fcb88dd3391b4c9ba41b8fe659ba7b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92460b5d1186430598630b3063b084a3",
            "placeholder": "​",
            "style": "IPY_MODEL_2e64b81aa01046328d4689b90daa6f5d",
            "value": " 990/990 [00:01&lt;00:00, 908.94 examples/s]"
          }
        },
        "5812012f4c0c4a43b0193a4a5c356e10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7941a1687234def8a3ebbc108e23697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d7606703c645e5babd5c705f3dc116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12ece3f91c204b2c88e89d282fb938a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fde3315841f4b65a201ef5f438067d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92460b5d1186430598630b3063b084a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e64b81aa01046328d4689b90daa6f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce6dc3fc2e2a4fd29bc5fe0909634521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e5d4aae3c3a4002af6fab3c82e57eb2",
              "IPY_MODEL_be976886eaea48e1994d2f16a534bfdc",
              "IPY_MODEL_c00e0cc8917a4c22a3c9522a2818f335"
            ],
            "layout": "IPY_MODEL_33bdc60ab44d44179e530f543c3fddef"
          }
        },
        "5e5d4aae3c3a4002af6fab3c82e57eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_441806314381481ab349f1dce38ef3a9",
            "placeholder": "​",
            "style": "IPY_MODEL_6311f599fe7c43bdb372b0c28eb32e15",
            "value": "Map: 100%"
          }
        },
        "be976886eaea48e1994d2f16a534bfdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc2b1d0df83345f881cea1a34dbb0230",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6748de8e783d424bb4e37daf384b2737",
            "value": 10
          }
        },
        "c00e0cc8917a4c22a3c9522a2818f335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f44ef6bfa904b468b774933384d48a2",
            "placeholder": "​",
            "style": "IPY_MODEL_b20ce80c417246a39803efd1fda77203",
            "value": " 10/10 [00:00&lt;00:00, 246.10 examples/s]"
          }
        },
        "33bdc60ab44d44179e530f543c3fddef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "441806314381481ab349f1dce38ef3a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6311f599fe7c43bdb372b0c28eb32e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc2b1d0df83345f881cea1a34dbb0230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6748de8e783d424bb4e37daf384b2737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f44ef6bfa904b468b774933384d48a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20ce80c417246a39803efd1fda77203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}